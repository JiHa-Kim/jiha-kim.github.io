- [ ] matrix spectral theory
- [ ] motivate Hilbert/Banach spaces



## Legend 
x=done
*=revise
1. (no prefix)=not started

## Crash courses
    * 1. `Numerical Analysis`
    2. `Tensor Calculus`, which leads to:
        1. `Differential Geometry` (which then, along with Statistics and Information Theory, leads to `Information Geometry`)
        2. `Statistics and Information Theory` (which then, along with Differential Geometry, leads to `Information Geometry`)

        3. `Convex Analysis` (which then leads to `Online Learning`)

## Series

x 1. Introduction to basic mathematical optimization
x 2. Iterative methods: gradient-free vs. gradient-based optimization
x 3. Desirable properties of optimizers
x 4. Speedrun of common gradient-based ML optimizers
x 5. Problem formalization
x 6. Gradient descent and gradient flow
x 7. Challenges of high-dimensional non-convex optimization in deep learning
x 8. Stochastic Gradient Descent and effects of randomness
x 9. Adaptive methods and preconditioning
x 10. Momentum
x 11. Soft inductive biases (regularization)
x 12. Adam optimizer, info geo view: diagonal Fisher information approximation
x 13. Adam optimizer, online learning view: Discounted Follow-The-Regularized-Leader
14. Metrized deep learning (Iso/IsoAdam, Shampoo, Muon)
15. Parameter-free optimization