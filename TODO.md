- [ ] momentum: compare uniform RMS-Prop to Adagrad up to dimension scaling factor
- [ ] soft inductive biases: example of unit disk opposite sides "shortcut"
- [ ] 



## Legend 
x=done
*=revise
1. (no prefix)=not started

## Crash courses
    * 1. `Numerical Analysis`
    2. `Tensor Calculus`, which leads to:
        1. `Differential Geometry` (which then, along with Statistics and Information Theory, leads to `Information Geometry`)
        2. `Statistics and Information Theory` (which then, along with Differential Geometry, leads to `Information Geometry`)

        3. `Convex Analysis` (which then leads to `Online Learning`)

## Series

x 1. Introduction to basic mathematical optimization
x 2. Iterative methods: gradient-free vs. gradient-based optimization
x 3. Desirable properties of optimizers
x 4. Speedrun of common gradient-based ML optimizers
5. Problem formalization
6. Gradient descent and gradient flow
7. Challenges of high-dimensional non-convex optimization in deep learning
8. Stochastic Gradient Descent and effects of randomness
9. Adaptive methods and preconditioning
x 10. Momentum
11. Soft inductive biases (regularization)
12. Adam optimizer, info geo view: diagonal Fisher information approximation
* 13. Adam optimizer, online learning view: Discounted Follow-The-Regularized-Leader
14. Metrized deep learning (Iso/IsoAdam, Shampoo, Muon)
15. Parameter-free optimization