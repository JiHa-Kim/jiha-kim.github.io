---
layout: collection-landing
title: "Online Learning Crash Course"
slug: online-learning-crash-course # Or just "online-learning" if you prefer shorter
description: "A foundational journey into sequential decision-making, regret minimization, and adaptive optimization algorithms."
cover: # placeholder (or specify an image path if you have one, e.g., /assets/img/covers/online-learning.jpg)
level: "Intermediate to Advanced" # Given the content (FTRL, OMD, regret analysis)
categories:
  - Machine Learning
  - Optimization Theory
tags: # Optional for collection landing, but can be useful for site-wide tagging
  - Online Learning
  - OCO
  - FTRL
  - Mirror Descent
  - Adaptive Optimization
  - Regret Minimization
---

This crash course provides a self-contained introduction to the fundamental principles and algorithms of online learning. We explore how to make optimal sequential decisions in the face of uncertainty, quantify performance using regret, and develop algorithms like Online Gradient Descent, Follow-The-Regularized-Leader, Mirror Descent, and adaptive methods such as AdaGrad.

The series emphasizes the mathematical underpinnings and geometric interpretations, laying the groundwork for understanding advanced optimization techniques in machine learning. Key topics include:

*   The Online Learning Protocol & Motivation
*   Regret Analysis & Performance Benchmarks
*   Gradient-Based Methods (OGD)
*   The Power of Regularization (FTRL)
*   Geometric Generalizations (Mirror Descent)
*   Adaptive Learning Rates (AdaGrad & Beyond)
*   Online-to-Batch Conversions & Generalization

This course is designed for readers comfortable with linear algebra, multivariable calculus, and basic mathematical notation. It serves as an essential prerequisite for deeper dives into optimization theory within machine learning, particularly for understanding adaptive optimizers and their connections to online methods.
